{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WalterPaixaoCortes/r3s-scripts/blob/main/notebooks/Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYUysU-5pPrt"
      },
      "source": [
        "# ECPA file download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTQ9zNqnFwMz"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hem7KqcEnEo1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import zipfile\n",
        "import gzip\n",
        "import traceback\n",
        "import glob\n",
        "import gc\n",
        "import logging\n",
        "import sqlite3\n",
        "import sys\n",
        "\n",
        "from sqlalchemy import event\n",
        "from sqlalchemy import create_engine\n",
        "from logging.handlers import TimedRotatingFileHandler\n",
        "from urllib.parse import urlparse\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import pandas as pd\n",
        "import requests as r\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Declaring auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_lines(file_name):\n",
        "    fp = open(file_name,'r', encoding=\"iso-8859-1\")\n",
        "    for line_count, line in enumerate(fp):\n",
        "        pass\n",
        "    return line_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the parameters for execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_folders = False\n",
        "download_files = False\n",
        "unzip_files = False\n",
        "use_sqlite = False\n",
        "clean_database = True\n",
        "save_to_database = True\n",
        "validate_process = True\n",
        "commit_size = 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzhDfwfZ5Wxm"
      },
      "source": [
        "## Defining the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fhandler = TimedRotatingFileHandler(\"logs/log.log\", when=\"midnight\", interval=1)\n",
        "fhandler.suffix = \"%Y%m%d\"\n",
        "logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "        handlers=[fhandler, logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build download URLS list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap9_fWGwpq4j"
      },
      "outputs": [],
      "source": [
        "download_urls = [\"https://echo.epa.gov/files/echodownloads/frs_downloads.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/case_downloads.zip\", \n",
        "                 \"https://echo.epa.gov/files/echodownloads/npdes_downloads.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/npdes_eff_downloads.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/npdes_master_general_permits.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/npdes_outfalls_layer.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/npdes_limits.zip\",\n",
        "                 \"https://echo.epa.gov/files/echodownloads/SDWA_latest_downloads.zip\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For TRI files, we need to add a sequence of files, since 1987."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tri_start = 1987\n",
        "tri_end = datetime.datetime.now().year -1\n",
        "tri_end_url = \"https://www3.epa.gov/tri/pds/US_%s.zip\"\n",
        "tri_url = \"https://www3.epa.gov/tri/current/US_%s.zip\"\n",
        "\n",
        "logger.info(f\"Loading URLs for TRI downloads from {tri_start} to {tri_end}...\")\n",
        "year = tri_start\n",
        "while year <= tri_end:\n",
        "  if year == tri_end:\n",
        "    url = tri_end_url % year\n",
        "  else:\n",
        "    url = tri_url % year\n",
        "  \n",
        "  download_urls.append(url)\n",
        "  year += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For WQI files, we need to detect the correct files on the folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_wqi_url = \"https://echo.epa.gov/files/echodownloads/Data-Analytics/WQI\"\n",
        "\n",
        "logger.info(f\"Loading URLs for WQI downloads...\")\n",
        "response = r.get(base_wqi_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "links = soup.find_all('a')\n",
        "\n",
        "for item in links:\n",
        "  if \"ResultFileToEnd2Output\" in item[\"href\"]:\n",
        "    download_urls.append(f'{base_wqi_url}/{item[\"href\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a special routine for DMR files as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dmr_url = \"https://echo.epa.gov/files/echodownloads\"\n",
        "\n",
        "logger.info(f\"Loading URLs for DMR downloads...\")\n",
        "response = r.get(base_dmr_url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "links = soup.find_all('a')\n",
        "\n",
        "for item in links:\n",
        "  if \"npdes_dmrs_\" in item[\"href\"]:\n",
        "    download_urls.append(f'{base_dmr_url}/{item[\"href\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zipfile_folder = \"zipfiles\"\n",
        "unzipped_folder = \"rawfiles\"\n",
        "database_folder = \"database\"\n",
        "extension = \".zip\"\n",
        "\n",
        "allowed_extensions = [\".txt\",\".csv\"]\n",
        "database_name = f\"{database_folder}/source.db\"\n",
        "\n",
        "my_conn = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, to help us out to not download files that were already downloaded, lets generate a list of downloaded files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "downloaded_files = []\n",
        "for item in os.listdir(zipfile_folder):\n",
        "  downloaded_files.append(os.path.basename(urlparse(item).path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCdaeWamFsTX"
      },
      "source": [
        "## Defining the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MXVLFcaE7sD",
        "outputId": "4b59d7b9-e1da-4792-9b77-e626bd67a320"
      },
      "outputs": [],
      "source": [
        "if create_folders:\n",
        "  if not os.path.exists(zipfile_folder): \n",
        "    os.mkdir(zipfile_folder)\n",
        "  if not os.path.exists(unzipped_folder): \n",
        "    os.mkdir(unzipped_folder)\n",
        "  if not os.path.exists(database_folder): \n",
        "    os.mkdir(database_folder)\n",
        "else:\n",
        "  logger.info(\"Folders already created...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX7cP_S2qUmh"
      },
      "outputs": [],
      "source": [
        "if use_sqlite and clean_database:\n",
        "  logger.info (\"Cleaning database to restart insert operation...\")\n",
        "  if os.path.exists(database_name):\n",
        "    if my_conn:\n",
        "      my_conn.close()\n",
        "    os.remove(database_name)\n",
        "else:\n",
        "  logger.info(\"Database will be used as is...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connecting or Creating database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_conn = None\n",
        "if use_sqlite:\n",
        "  my_conn=sqlite3.connect(database_name)\n",
        "else:\n",
        "  logger.info(os.getenv(\"PG_DATA_CONN\"))\n",
        "  my_conn = create_engine(os.getenv(\"PG_DATA_CONN\"))  \n",
        "logger.info(\"Connected to database...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRrvb3U21_lq"
      },
      "source": [
        "## Download zip files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvysEExu1zsp",
        "outputId": "aec3bbec-d75c-4784-f252-779f41b6df47"
      },
      "outputs": [],
      "source": [
        "if download_files:\n",
        "  logger.info(f\"Starting download process. Total files to be downloaded: {len(download_urls)}...\")\n",
        "  for download_url in download_urls:\n",
        "    file_name = os.path.basename(urlparse(download_url).path)\n",
        "    if file_name not in downloaded_files:\n",
        "      logger.info(f\"Downloading file {file_name}...\")\n",
        "      try:\n",
        "        response = r.get(download_url, allow_redirects=True)\n",
        "        with open(os.path.join(zipfile_folder, file_name), \"wb\") as fw:\n",
        "          fw.write(response.content)\n",
        "          logger.info(f\"--> File {file_name} saved.\")\n",
        "      except:\n",
        "          logger.error(f\"--> File {file_name} not downloaded.\")\n",
        "else:\n",
        "  logger.info(\"Files already downloaded...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24JS1_z2yC3"
      },
      "source": [
        "## Unzip the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh4q-wLe27qR",
        "outputId": "401fbb8c-9528-4d7f-c0e8-e2e9dbc8d383"
      },
      "outputs": [],
      "source": [
        "if unzip_files:\n",
        "  for item in os.listdir(zipfile_folder):\n",
        "    if item.endswith(extension) and item not in downloaded_files: \n",
        "      logger.info(f\"Unzipping file {item}...\")\n",
        "      try:\n",
        "        file_name = os.path.abspath(os.path.join(zipfile_folder, item)) \n",
        "        zip_ref = zipfile.ZipFile(file_name)\n",
        "        zip_ref.extractall(unzipped_folder)\n",
        "        zip_ref.close()\n",
        "        logger.info(f\"--> File {item} unzipped.\")\n",
        "      except:\n",
        "        logger.error(f\"--> File {item} not unzipped.\")\n",
        "    elif item.endswith(\".gz\")  and item not in downloaded_files:\n",
        "      logger.info(f\"Decompressing file {item}...\")\n",
        "      try:\n",
        "        file_name = os.path.abspath(os.path.join(zipfile_folder, item)) \n",
        "        new_file_name = os.path.abspath(os.path.join(unzipped_folder, item.replace(\".gz\",\"\"))) \n",
        "        file_out = gzip.decompress(open(file_name, 'rb').read())\n",
        "        with open(new_file_name, 'wb') as fw:\n",
        "          fw.write(file_out)\n",
        "        logger.info(f\"File {new_file_name} decompressed and saved...\")        \n",
        "      except:\n",
        "        logger.error(f\"--> File {item} not decompressed.\")\n",
        "    else:\n",
        "      logger.info(f\"Skipping file {item}.\")\n",
        "else:\n",
        "  logger.info(\"Files already unzipped...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save to database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWu3qqSva92k",
        "outputId": "c059275a-2e5a-4c23-d5e8-5545405d3abf"
      },
      "outputs": [],
      "source": [
        "if save_to_database:\n",
        "  logger.info(\"Preparing list of files to be processed...\")\n",
        "  list_of_files = filter(os.path.isfile, glob.glob(unzipped_folder + '/*') )\n",
        "  list_of_files = sorted(list_of_files, key =  lambda x: os.stat(x).st_size)  \n",
        "  files = [os.path.basename(item) for item in list_of_files]\n",
        "  \n",
        "  for item in files:\n",
        "    table_name, file_ext = os.path.splitext(os.path.basename(item))\n",
        "    df = None\n",
        "    if file_ext in allowed_extensions and not table_name.startswith(\"ResultFile\"):\n",
        "      second = False\n",
        "      if not table_name.startswith(\"US\"):\n",
        "        try:\n",
        "          for df in pd.read_csv(os.path.join(unzipped_folder, item), encoding=\"iso-8859-1\", index_col=False, chunksize=commit_size, dtype=str, on_bad_lines=\"skip\", encoding_errors=\"replace\"):\n",
        "            df.to_sql(table_name, my_conn, schema=\"source\",\n",
        "                    if_exists=\"append\", \n",
        "                    index=False)\n",
        "          logger.info(f\"File {item} saved on the database...\")\n",
        "        except:\n",
        "          logger.error(traceback.format_exc())\n",
        "          second = True\n",
        "      else:\n",
        "        second = True\n",
        "        \n",
        "      if second:\n",
        "        try:      \n",
        "          for df in pd.read_csv(os.path.join(unzipped_folder, item), encoding=\"iso-8859-1\", sep=\"\\t\", index_col=False, chunksize=commit_size, dtype=str, on_bad_lines=\"skip\", encoding_errors=\"replace\"):\n",
        "            df.to_sql(table_name, my_conn,  schema=\"source\",\n",
        "                    if_exists=\"append\", \n",
        "                    index=False)\n",
        "          logger.info(f\"File {item} saved on the database...\")\n",
        "        except:\n",
        "          logger.error(f\"File {item} not saved on the database...\")\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "  logger.info(\"Database already loaded...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validating Load Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will check based on line counts if the process to send to the database was sucessful or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if validate_process:\n",
        "  success_data = { \"File\": [], \"File Lines\": [], \"Table\": [], \"Table Rows\": [], \"Difference\": []}\n",
        "  error_data = { \"File\": [], \"File Lines\": [], \"Table\": [], \"Table Rows\": [], \"Difference\": []}\n",
        "\n",
        "  logger.info(\"Preparing list of files to be processed...\")\n",
        "  list_of_files = filter(os.path.isfile, glob.glob(unzipped_folder + '/*') )\n",
        "  list_of_files = sorted(list_of_files, key =  lambda x: os.stat(x).st_size)  \n",
        "  files = [os.path.basename(item) for item in list_of_files]\n",
        "  for item in files:\n",
        "    try:\n",
        "      file_name = os.path.join(unzipped_folder, item)\n",
        "      table_name, file_ext = os.path.splitext(os.path.basename(item))\n",
        "      logger.info(f\"{file_name} and {table_name} being compared...\")\n",
        "      if file_ext in allowed_extensions:\n",
        "        file_count = count_lines(file_name)\n",
        "        db_count = my_conn.execute(f\"select count(*) from {table_name}\").fetchone()[0]\n",
        "        diff = file_count - db_count\n",
        "        if diff > 0:\n",
        "          error_data[\"File\"].append(file_name)\n",
        "          error_data[\"File Lines\"].append(file_count)\n",
        "          error_data[\"Table\"].append(table_name)\n",
        "          error_data[\"Table Rows\"].append(db_count)\n",
        "          error_data[\"Difference\"].append(diff)\n",
        "        else:\n",
        "          success_data[\"File\"].append(file_name)\n",
        "          success_data[\"File Lines\"].append(file_count)\n",
        "          success_data[\"Table\"].append(table_name)\n",
        "          success_data[\"Table Rows\"].append(db_count)\n",
        "          success_data[\"Difference\"].append(diff)\n",
        "    except:\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "  logger.info(\"Saving results as files...\")\n",
        "  error_report = pd.DataFrame(error_data)\n",
        "  error_report.to_markdown(os.path.join(database_folder, \"issues.md\"))\n",
        "\n",
        "  success_report = pd.DataFrame(success_data)\n",
        "  success_report.to_markdown(os.path.join(database_folder, \"success.md\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closing the Database Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "9-W2GCRhv0xE",
        "outputId": "cf40c9c8-b8ad-433c-888d-26a1dd56574c"
      },
      "outputs": [],
      "source": [
        "my_conn.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Data Exploration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "cb3e5365f865ba04769fee69367f1bd90706cdadd21dbb2d03785d9fb41b8a9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
